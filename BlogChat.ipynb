{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdayG01/BlogChat/blob/main/BlogChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cQhAWYu-MFZQ"
      },
      "outputs": [],
      "source": [
        "! pip install -q unstructured chromadb huggingface_hub sentence_transformers langchain\n",
        "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -q llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-IHKUceCLaJi"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import CTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lNMoAuBJMEpb"
      },
      "outputs": [],
      "source": [
        "urls = [\"https://blog.aquasec.com/powerhell-active-flaws-in-powershell-gallery-expose-users-to-attacks\"]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "# for doc in data:\n",
        "#   print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsnlPXQ9NycT"
      },
      "outputs": [],
      "source": [
        "# loader2 = WebBaseLoader(\"https://blog.aquasec.com/powerhell-active-flaws-in-powershell-gallery-expose-users-to-attacks\")\n",
        "# data2 = loader2.load()\n",
        "# data2\n",
        "\n",
        "# for doc in data2:\n",
        "#   print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGUAxsGPgX1"
      },
      "outputs": [],
      "source": [
        "# currently I've used UnstructuredURLLoader however I'll be attempting to use the SeleniumURLLoader as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UeNy3EkJQcuc"
      },
      "outputs": [],
      "source": [
        "# Performing text splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=10)\n",
        "\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kCD_q-I3JWTt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7vTCBL7RJpIN"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "# embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-IiHENkMKI3U"
      },
      "outputs": [],
      "source": [
        "# ! pip install pydantic==1.9.0\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsFAzp4ZjsFz"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# repo_id = \"TheBloke/Llama-2-13B-GGML\"\n",
        "# llm = HuggingFaceHub(huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "#                      repo_id=repo_id,\n",
        "#                      model_kwargs={\"temperature\":0.6, \"max_new_tokens\":500})\n",
        "\n",
        "\n",
        "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "model_basename = \"llama-2-7b-chat.Q2_K.gguf\"\n",
        "\n",
        "# config = {'max_new_tokens': 350, 'context_length': 1024}\n",
        "# llm = CTransformers(model=model_name_or_path, model_file=model_basename, callbacks=[StreamingStdOutCallbackHandler()], config=config)\n",
        "\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# GPU\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# CPU\n",
        "# llm = LlamaCpp(\n",
        "#     model_path=model_path,\n",
        "#     temperature=0.75,\n",
        "#     max_tokens=2000,\n",
        "#     top_p=1,\n",
        "#     verbose=True, # Verbose is required to pass to the callback manager\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q transformers einops accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gibjSJuslFh",
        "outputId": "339fa3b0-41fd-4a14-8e6a-b68658bf7bb6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m868.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain import HuggingFacePipeline\n",
        "# from transformers import AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "# model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# pipeline = pipeline(\n",
        "#     \"text-generation\", #task\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     trust_remote_code=True,\n",
        "#     device_map=\"auto\",\n",
        "#     max_length=200,\n",
        "#     do_sample=True,\n",
        "#     top_k=10,\n",
        "#     num_return_sequences=1,\n",
        "#     eos_token_id=tokenizer.eos_token_id\n",
        "# )\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "RgZ4BnyVsNyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHv0FyRKZN1P"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UCwt2gaZIw0"
      },
      "outputs": [],
      "source": [
        "# from langchain import HuggingFacePipeline\n",
        "# from transformers import AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "# model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# pipeline = pipeline(\n",
        "#     \"text-generation\", #task\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     trust_remote_code=True,\n",
        "#     device_map=\"auto\",\n",
        "#     max_length=200,\n",
        "#     do_sample=True,\n",
        "#     top_k=10,\n",
        "#     num_return_sequences=1,\n",
        "#     eos_token_id=tokenizer.eos_token_id\n",
        "# )\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9nai38a-Dyq"
      },
      "source": [
        "```\n",
        "ValidationError: 1 validation error for LlamaCpp\n",
        "__root__\n",
        "  Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-7B-Uncensored-GGML/snapshots/531879da598ebc577cd4a03bdde9fbe3a641fc63/Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_S.bin. Received error  (type=value_error)\n",
        "```\n",
        "This is the error that I've been receiving on running LLamaCpp, I believe there's some error in importing the GGML model from the hugging face library, I'll be attempting to solve this and in the interim will be employing another potential LLM for the use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbkWHkQbhDzB"
      },
      "source": [
        "https://python.langchain.com/docs/use_cases/question_answering/\n",
        "\n",
        "* read this before you complete the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQlSPRhWvJ3B"
      },
      "outputs": [],
      "source": [
        "def getResponse(query):\n",
        "  docs = db.similarity_search(query, k=4)\n",
        "\n",
        "  # question_prompt_template = \"\"\"Use the document provided to answer the question asked by the user. Keep your answer brief and precise, answer in about 200-300 words.\n",
        "  # if you don't know the answer say that you don't know, do not make up an answer\n",
        "  # {context}\n",
        "\n",
        "  # Question: {question}\"\"\"\n",
        "\n",
        "  # prompt = PromptTemplate(\n",
        "  #     template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        "  # )\n",
        "\n",
        "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "  return (chain.run(input_documents=docs, question=query))\n",
        "\n",
        "\n",
        "sample_query = 'what are the flaws discussed in the blog provided to you? Also name those flaws'\n",
        "response = getResponse(sample_query)\n",
        "response"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXHihPVMyDTeJXFSC1NmvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}