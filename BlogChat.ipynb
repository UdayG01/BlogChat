{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdayG01/BlogChat/blob/main/BlogChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQhAWYu-MFZQ"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install ctransformers\n",
        "! pip install unstructured\n",
        "! pip install chromadb\n",
        "! pip install huggingface_hub\n",
        "! pip install sentence_transformers\n",
        "\n",
        "# ! pip install openai\n",
        "# ! pip install tiktoken\n",
        "# ! pip install pydantic\n",
        "# ! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "# ! pip install transformers --quiet\n",
        "# ! pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-IHKUceCLaJi"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import CTransformers\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lNMoAuBJMEpb"
      },
      "outputs": [],
      "source": [
        "urls = [\"https://blog.aquasec.com/powerhell-active-flaws-in-powershell-gallery-expose-users-to-attacks\"]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "# for doc in data:\n",
        "#   print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsnlPXQ9NycT"
      },
      "outputs": [],
      "source": [
        "# loader2 = WebBaseLoader(\"https://blog.aquasec.com/powerhell-active-flaws-in-powershell-gallery-expose-users-to-attacks\")\n",
        "# data2 = loader2.load()\n",
        "# data2\n",
        "\n",
        "# for doc in data2:\n",
        "#   print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGUAxsGPgX1"
      },
      "outputs": [],
      "source": [
        "# currently I've used UnstructuredURLLoader however I'll be attempting to use the SeleniumURLLoader as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UeNy3EkJQcuc"
      },
      "outputs": [],
      "source": [
        "# Performing text splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=10)\n",
        "\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kCD_q-I3JWTt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7vTCBL7RJpIN"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "# embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-IiHENkMKI3U"
      },
      "outputs": [],
      "source": [
        "# ! pip install pydantic==1.9.0\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vsFAzp4ZjsFz"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# from langchain.llms import HuggingFaceHub\n",
        "# from langchain.llms import Petals\n",
        "# from langchain.llms import LlamaCpp\n",
        "# from huggingface_hub import hf_hub_download\n",
        "# from langchain.llms import OpenAI\n",
        "\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "repo_id = \"tiiuae/falcon-7b\"\n",
        "llm = HuggingFaceHub(huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "                     repo_id=repo_id,\n",
        "                     model_kwargs={\"temperature\":0.6, \"max_new_tokens\":500})\n",
        "\n",
        "\n",
        "# model_name_or_path = \"TheBloke/LLaMa-7B-GGML\"\n",
        "# model_basename = \"llama-7b.ggmlv3.q2_K.bin\"\n",
        "\n",
        "# config = {'max_new_tokens': 350, 'context_length': 1024}\n",
        "# llm = CTransformers(model=model_name_or_path, model_file=model_basename, callbacks=[StreamingStdOutCallbackHandler()], config=config)\n",
        "\n",
        "\n",
        "# model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# GPU\n",
        "# llm = LlamaCpp(\n",
        "#     model_path=model_path,\n",
        "#     n_gpu_layers=n_gpu_layers,\n",
        "#     n_batch=n_batch,\n",
        "#     verbose=True,\n",
        "# )\n",
        "\n",
        "# CPU\n",
        "# llm = LlamaCpp(\n",
        "#     model_path=model_path,\n",
        "#     temperature=0.75,\n",
        "#     max_tokens=2000,\n",
        "#     top_p=1,\n",
        "#     verbose=True, # Verbose is required to pass to the callback manager\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHv0FyRKZN1P"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UCwt2gaZIw0"
      },
      "outputs": [],
      "source": [
        "# from langchain import HuggingFacePipeline\n",
        "# from transformers import AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "# model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# pipeline = pipeline(\n",
        "#     \"text-generation\", #task\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     trust_remote_code=True,\n",
        "#     device_map=\"auto\",\n",
        "#     max_length=200,\n",
        "#     do_sample=True,\n",
        "#     top_k=10,\n",
        "#     num_return_sequences=1,\n",
        "#     eos_token_id=tokenizer.eos_token_id\n",
        "# )\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9nai38a-Dyq"
      },
      "source": [
        "```\n",
        "ValidationError: 1 validation error for LlamaCpp\n",
        "__root__\n",
        "  Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-7B-Uncensored-GGML/snapshots/531879da598ebc577cd4a03bdde9fbe3a641fc63/Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_S.bin. Received error  (type=value_error)\n",
        "```\n",
        "This is the error that I've been receiving on running LLamaCpp, I believe there's some error in importing the GGML model from the hugging face library, I'll be attempting to solve this and in the interim will be employing another potential LLM for the use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbkWHkQbhDzB"
      },
      "source": [
        "https://python.langchain.com/docs/use_cases/question_answering/\n",
        "\n",
        "* read this before you complete the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fQlSPRhWvJ3B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "5b35d10c-68c8-4aef-d5d6-fada15f16530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe first flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery. The vulnerability of the PowerShell Gallery makes it possible for an attacker to create a script that could be hosted on the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery. The vulnerability of the PowerShell Gallery makes it possible for an attacker to create a script that could be hosted on the PowerShell Gallery.\\n\\nThe second flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe third flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe fourth flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe fifth flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe sixth flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe seventh flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe eighth flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell Gallery.\\n\\nThe ninth flaw is the one highlighted in the title of the blog. The flaw is associated with the vulnerability of the PowerShell Gallery. It is possible for an attacker to create a malicious script that could be hosted on the PowerShell'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def getResponse(query):\n",
        "  docs = db.similarity_search(query, k=4)\n",
        "\n",
        "  # question_prompt_template = \"\"\"Use the document provided to answer the question asked by the user. Keep your answer brief and precise, answer in about 200-300 words.\n",
        "  # if you don't know the answer say that you don't know, do not make up an answer\n",
        "  # {context}\n",
        "\n",
        "  # Question: {question}\"\"\"\n",
        "\n",
        "  # prompt = PromptTemplate(\n",
        "  #     template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        "  # )\n",
        "\n",
        "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "  return (chain.run(input_documents=docs, question=query))\n",
        "\n",
        "\n",
        "sample_query = 'what are the flaws discussed in the blog provided to you? Also name those flaws'\n",
        "response = getResponse(sample_query)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3SGY1YSq129"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit\n",
        "\n",
        "! pip install langchain\n",
        "! pip install ctransformers\n",
        "! pip install unstructured\n",
        "! pip install chromadb\n",
        "! pip install huggingface_hub\n",
        "! pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD0rNAvfq77T",
        "outputId": "41c490fb-08ed-42c8-d8b5-03d69841240c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# from huggingface_hub import hf_hub_download\n",
        "# from langchain.llms import LlamaCpp\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "from langchain.document_loaders import UnstructuredURLLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import CTransformers\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "from htmlTemplates import css, bot_template, user_template\n",
        "\n",
        "import os\n",
        "huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "repo_id = \"tiiuae/falcon-7b\"\n",
        "llm = HuggingFaceHub(huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "                     repo_id=repo_id,\n",
        "                     model_kwargs={\"temperature\":0, \"max_new_tokens\":350})\n",
        "\n",
        "def get_blog_text(blog_url):\n",
        "  text = \"\"\n",
        "  urls = [blog_url]\n",
        "  loader = UnstructuredURLLoader(urls=urls)\n",
        "  data = loader.load()\n",
        "\n",
        "  for doc in data:\n",
        "    text += doc.page_content\n",
        "  return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "   text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=10)\n",
        "   docs = text_splitter.split_text(text)\n",
        "   return docs\n",
        "\n",
        "def get_vectorstore(docs):\n",
        "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "  db = Chroma.from_texts(docs, embeddings)\n",
        "  return db\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "  # n_gpu_layers = 40\n",
        "  # n_batch = 512\n",
        "\n",
        "  # Loading model,\n",
        "  # llm = LlamaCpp(\n",
        "  #     model_path=model_path,\n",
        "  #     max_tokens=256,\n",
        "  #     n_gpu_layers=n_gpu_layers,\n",
        "  #     n_batch=n_batch,\n",
        "  #     n_ctx=2048,\n",
        "  #     verbose=False,\n",
        "  # )\n",
        "\n",
        "  # repo_id = \"google/flan-t5-xxl\"\n",
        "  # llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64})\n",
        "\n",
        "  memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages=True)\n",
        "  conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=vectorstore.as_retriever(),\n",
        "      memory=memory\n",
        "  )\n",
        "  return conversation_chain\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "  response = st.session_state.conversation({'question': user_question})\n",
        "  st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "  for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "  st.set_page_config(page_title=\"BlogChat\",\n",
        "                    page_icon=\":wind_blowing_face:\")\n",
        "  st.write(css, unsafe_allow_html=True)\n",
        "\n",
        "  if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = None\n",
        "  if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = None\n",
        "\n",
        "\n",
        "  st.header(\"Chat with Blog :pencil:\")\n",
        "  user_question = st.text_input(\"Query about your blog \")\n",
        "  if user_question:\n",
        "    handle_userinput(user_question)\n",
        "\n",
        "  with st.sidebar:\n",
        "    st.subheader(\"Your URLs\")\n",
        "    with st.form(key='blog-url-form'):\n",
        "      # Input for the blog URL\n",
        "      blog_url = st.text_input(label='Enter the blog URL')\n",
        "      submit_button = st.form_submit_button(label='Submit')\n",
        "\n",
        "    if submit_button:\n",
        "      # with st.spinner(\"Process\"):\n",
        "        # Perform loading on the pdf\n",
        "        myBar = st.progress(0, text=\"Processing...\")\n",
        "\n",
        "        text = get_blog_text(blog_url)\n",
        "        myBar.progress(25, text=\"Processing...\")\n",
        "\n",
        "        # Perform text-splitting\n",
        "        docs  = get_text_chunks(text)\n",
        "        myBar.progress(50, text=\"Processing...\")\n",
        "\n",
        "        # Performing embeddings/vectorization using HuggingFaceEmbeddings\n",
        "        # and I'll be storing these embeddings in Chroma vector store\n",
        "        db = get_vectorstore(docs)\n",
        "        myBar.progress(75, text=\"Processing...\")\n",
        "\n",
        "        # Making a conversation chain\n",
        "        # conversation = get_conversation_chain(db)\n",
        "        # do the below one in case you want a persistent convo\n",
        "        st.session_state.conversation = get_conversation_chain(db)\n",
        "        myBar.progress(100, text=\"Done\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPiOQbQdq19t",
        "outputId": "3cea8ae2-876c-4178-914e-14db0bc9c1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting htmlTemplates.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile htmlTemplates.py\n",
        "\n",
        "\n",
        "css = '''\n",
        "<style>\n",
        ".chat-message {\n",
        "    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\n",
        "}\n",
        ".chat-message.user {\n",
        "    background-color: #2b313e\n",
        "}\n",
        ".chat-message.bot {\n",
        "    background-color: #475063\n",
        "}\n",
        ".chat-message .avatar {\n",
        "  width: 20%;\n",
        "}\n",
        ".chat-message .avatar img {\n",
        "  max-width: 78px;\n",
        "  max-height: 78px;\n",
        "  border-radius: 50%;\n",
        "  object-fit: cover;\n",
        "}\n",
        ".chat-message .message {\n",
        "  width: 80%;\n",
        "  padding: 0 1.5rem;\n",
        "  color: #fff;\n",
        "}\n",
        "'''\n",
        "\n",
        "bot_template = '''\n",
        "<div class=\"chat-message bot\">\n",
        "    <div class=\"avatar\">\n",
        "        Falcon-7b\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "user_template = '''\n",
        "<div class=\"chat-message user\">\n",
        "    <div class=\"avatar\">\n",
        "        You\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3dPt4bdh_p9Q"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp06VMqR_r76",
        "outputId": "4468c8e6-c9f4-4687-88c1-2629dfde77c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.337s\n",
            "your url is: https://witty-dots-melt.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9XvhxvDnWMuag2yfciRmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}